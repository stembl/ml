{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Representation\n",
    "\n",
    "## Classification\n",
    "* Examples\n",
    "    * Email: Spam / Not Spam\n",
    "    * Transaction Fraud: Yes / No\n",
    "* Usually applying linear regression to a classification problem will not provide good results\n",
    "\n",
    "### Binary Classification Problem\n",
    "$y = {0, 1}$\n",
    "* 0 ~ Negative Class, generally conveys the abscense of something\n",
    "* 1 ~ Positive Class\n",
    "* Linear regression returns a value not bounded by 0 and 1.  Logistic Regression is used instead for classification problems.\n",
    "\n",
    "\n",
    "\n",
    "## Hypothesis Representation\n",
    "* The Hypothesis representation of Logistic Regression.\n",
    "* Sigmoid and Logistic function labels are interchangable\n",
    "* This model is bounded by 0 and 1\n",
    "* Interpretation: the result of $h_\\theta(X)$ is the probability the result is 1.  More formally $h_\\theta(x) = P(y=1|x;\\theta)$\n",
    "* Since $y = 0;1$, $P(y=0|x;\\theta) = 1 - P(y=1|x;\\theta)$\n",
    "\n",
    "$ 0 \\leq h_\\theta(x) \\leq 1 $\n",
    "\n",
    "Use: $h_\\theta(x) = g(\\theta^T x)$, where $g(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "This leads to: $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}$\n",
    "\n",
    "## Decision Boundary\n",
    "* Understand better what the hypothesis function looks like\n",
    "* Suppose: predect $y=1$ if $h_\\theta(x) \\geq 0.5$ and $y=0$ for $h_\\theta(x) < 0.5$.\n",
    "    * then $g(z) \\geq 0.5$ when $z \\geq 0$\n",
    "    * leads to $y=1$ when $\\theta^T x \\geq 0$ and $y=0$ when $\\theta^T x < 0$\n",
    "* The 'decision boundary' separates the $y=0$ and $y=1$ values.\n",
    "* The decision boundary is a property of the hypothesis and not the data set.\n",
    "* The decision boundary does not need to be linear.  This can be accomplished by adding higher order features.  For example: $h_\\theta(x) = g(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_1^2 + \\theta_4 x_2^2)$\n",
    "\n",
    "\n",
    "# Logistic Regression Model\n",
    "The same cost function can not be used from the Linear regression model.\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "$\\begin{align*}\n",
    "& J(\\theta) = \\dfrac{1}{m} \\sum_{i=1}^m \\mathrm{Cost}(h_\\theta(x^{(i)}),y^{(i)}) \\\\\n",
    "& \\mathrm{Cost}(h_\\theta(x),y) = -\\log(h_\\theta(x)) \\; & \\text{if y = 1} \\\\ \n",
    "& \\mathrm{Cost}(h_\\theta(x),y) = -\\log(1-h_\\theta(x)) \\; & \\text{if y = 0}\n",
    "\\end{align*}$\n",
    "\n",
    "If $y=1$ the cost goes to $0$ as $h_\\theta(x)$ approaches $1$ and if $y=0$ the cost goes to $\\inf$ as $h_\\theta(x)$ approaches $1$. \n",
    "\n",
    "$\\begin{align*}\n",
    "& \\mathrm{Cost}(h_\\theta(x),y) = 0 \\text{ if } h_\\theta(x) = y \\newline \n",
    "& \\mathrm{Cost}(h_\\theta(x),y) \\rightarrow \\infty \\text{ if } y = 0 \\; \\mathrm{and} \\; h_\\theta(x) \\rightarrow 1 \\newline \n",
    "& \\mathrm{Cost}(h_\\theta(x),y) \\rightarrow \\infty \\text{ if } y = 1 \\; \\mathrm{and} \\; h_\\theta(x) \\rightarrow 0 \\newline \n",
    "\\end{align*}$\n",
    "\n",
    "Note that writing the cost function in this way guarantees that $J(\\theta)$ is convex for logistic regression.\n",
    "\n",
    "## Simplified Cost Function\n",
    "\n",
    "## Advanced Optimization\n",
    "\n",
    "# Multiclass Classification\n",
    "\n",
    "## One-vs-All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
