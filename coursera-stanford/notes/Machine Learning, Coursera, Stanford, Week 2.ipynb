{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week Two - 20170410\n",
    "# Linear Regression with Multiple Variables\n",
    "A new version of linear version that is more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "Notation\n",
    "* $m$ ~ number of training examples \n",
    "* $n = |x^{(i)}|$ ~ number of features\n",
    "* $x^{(i)}$ ~ input (features) of the $i^{th}$ training example\n",
    "* $x_j^{(i)}$ ~ value of the feature $j$ in the $i^{th}$ training example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Features\n",
    "For convenience of notation, define $x_0 = 1$ ($x_0^{(i)} = 1$).\n",
    "\n",
    "$\\begin{align*}h_\\theta(x) =\\begin{bmatrix}\\theta_0 \\hspace{2em} \\theta_1 \\hspace{2em} ... \\hspace{2em} \\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\newline x_1 \\newline \\vdots \\newline x_n\\end{bmatrix}= \\theta^T x\\end{align*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Multiple Variables\n",
    "Repeat until convergence:\n",
    "\n",
    "{\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits^m_{i=1} ((h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}$ for $j:=0,...,n$\n",
    "\n",
    "}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling\n",
    "* You want your features to be on a similar scale.  If the feature scale has a large difference, it can result in a cost functions with a skewed contour shape increasing the solution time.\n",
    "* Generally try to get every feature to fall in the range $-1 \\leq x_i \\leq 1$.\n",
    "* Problems generally only arrise if the features lie on a different order of magnitude.\n",
    "* Mean normalization is when you replace $x_i$ with $x_i - \\mu_i$ to make the features have approximately zero mean (does not apply to $x_0 = 1$).\n",
    "\n",
    "In general:\n",
    "\n",
    "$x_1 \\leftarrow \\frac{x_1 - \\mu_1}{S_1},$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mu_j$ is the average value of $x_j$ in the training set\n",
    "* $S_j$ is the range (max - min) or standard deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate\n",
    "* How to 'debug' the learning rate so that it is opperating correctly.\n",
    "* $J(\\theta)$ should decrease after every iteration. Ploting number of iterations vs min $J(\\theta)$ will illustrate if the system is behaving correctly.  It will also illustrate when $J(\\theta)$ is not decreaseing any more.\n",
    "* It can be difficult to tell how many iterations will be required for the algorithm beforehand to converge. Plotting the aove will help.  Choosing the threshold at which $J(\\theta)$ min change is difficult.  \n",
    "* If $J(\\theta)$ is increasing, use a smaller $\\alpha$\n",
    "* If $J(\\theta)$ has a saw tooth shape, use a smaller $\\alpha$\n",
    "* If $\\alpha$ is to small, $J(\\theta)$ will be slow to converge\n",
    "* To choose $\\alpha$ try $..., 0.001, 0.01, 0.1, 1.0, ...$  Find a value that works and refine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Polynomial Regression\n",
    "* Defining new features may lead to a better model.  ie. area = frontage x depth\n",
    "* Polynomial regression, is the selection of the appropriate order to fit the data.\n",
    "* Example Price vs Size(x), if we believe the size is a cubic function.\n",
    "\n",
    "$\\begin{align}\n",
    "h_\\theta(x)= & \\theta_0+\\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3\\\\\n",
    "           = & \\theta_0 + \\theta_1(size) + \\theta_2(size)^2+\\theta_3(size)^3\\\\\n",
    "        x_1= & (size)\\\\\n",
    "        x_2= & (size)^2\\\\\n",
    "        x_3= & (size)^3\n",
    "\\end{align}$\n",
    "\n",
    "* Another option may be $h_\\theta(x) = \\theta_0 + \\theta_1(size) + \\theta_2\\sqrt{(size)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Parameters Analytically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations\n",
    "* Normal equation allows you to solve for $\\theta$ analytically.  \n",
    "* Normally, minimize a quadratic function by setting the derivative equal to zero.  Solve for $\\theta$.\n",
    "* Minimizing a cost function of a $n+1$ parameter take the partial of the cost function and set it equal to zero.  Solver for all $\\theta_n$.\n",
    "* Feature scaling is not required when using the Normal Equation Method.\n",
    "\n",
    "\n",
    "$ \\theta = (X^T X)^{-1} X^T y$\n",
    "\n",
    "Octave:\n",
    "* pinv(X'*X)*X'*y, pinv() calculates the inverse of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.05045320e+01]\n",
      " [  4.55787370e+02]\n",
      " [  1.02233854e+03]\n",
      " [  1.00420330e+00]\n",
      " [  1.54601059e+03]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "# Example Training Set\n",
    "m = 4\n",
    "\n",
    "X = pd.DataFrame({\n",
    "        'x0' : 1.,\n",
    "        'Size': sp.array([2104, 1416, 1534, 852],dtype='float32'),\n",
    "        'Beds': sp.array([5,3,3,2],dtype='int32'),\n",
    "        'Floors': sp.array([1,2,2,1],dtype='int32'),\n",
    "        'Age': sp.array([45,40,30,36],dtype='int32')\n",
    "    })\n",
    "\n",
    "y = pd.DataFrame({'Price': sp.array([460, 232, 315, 178],dtype='float32')})\n",
    "\n",
    "theta = sp.dot(sp.dot(sp.dot(X.T, X)**-1, X.T),y)\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing between Gradient Descent and Normal Equation\n",
    "\n",
    "Assume: $m$ training examples and $n$ features.\n",
    "    \n",
    "Gradient Descent\n",
    "* Need to choose $\\alpha$\n",
    "* Needs many iteratiosn\n",
    "* Works well when $n$ is large\n",
    "* Has complexity $\\mathcal{O}(k n^2)$\n",
    "\n",
    "Normal Equation\n",
    "* No need to choose $\\alpha$\n",
    "* Do not need to interate\n",
    "* Need to compute $(X^T X)^{-1})$, and $n$ x $n$ matrix.\n",
    "    * Has complexity $\\mathcal{O}(n^3)$\n",
    "    * With modern computers, n = 10000 is the point at which selecting Gradient Descent may be preferable.\n",
    "* Slow if $n$ is very large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation and Noninvertibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What if $X^T X$ is non-invertible?\n",
    "* In Octave when using the pinv() vs inv() the pinv() function should calculate the correct $\\theta$ even if the problem is non-invertible.\n",
    "* When two features have the same $\\theta$ (redundant), the problem should prove to be non-invertible.  ie if features include $x_1$ = size in feet$^2$ and $x_2$ = size in meters$^2$.\n",
    "* Also may occur when there are to many features (e.g. $m\\leq n$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
