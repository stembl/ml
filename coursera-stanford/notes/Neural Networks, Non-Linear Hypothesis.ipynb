{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Non-Linear Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Hypothesis\n",
    "* Adding polynomials to fit complex shapes works well when you have 2 features.  It may not work well with multiple features. For example n = 100 -> 5000 terms with just second order considered.\n",
    "* Image analysis needs non linear hypothesis.  \n",
    "* Large n's lead to to many features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons and the Brain\n",
    "* \"one learning algorithm\" hypothesis, validated by neuro-rewiring experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Representation\n",
    "* Modeled off networked neurons in the brain.\n",
    "* Neuron modeled as a logistic unit.\n",
    "* output computed as \n",
    "$h_\\theta (x) = \\frac{1}{1 + \\exp^{\\theta^T X}}$\n",
    "* may be influenced by a biased unit, $x_0 = 1$\n",
    "* Neural network example: Layer 1 ~ Input layer, Layer 2 ~ Hidden layer, Layer 3 ~ output layer.\n",
    "\n",
    "Notation\n",
    "* $i$ ~ index, unit in each layer\n",
    "* $j$ ~ index, layers\n",
    "* $s_j$ ~ number of activation nodes\n",
    "* $a_i^j$ ~ \"activation\" of unit i in layer j\n",
    "* $\\Theta^j$ ~ matrix of weights controling function mapping from layer $j$ to layer $j+1$.\n",
    "* $g$ ~ sigmoid $g(z) = \\frac{1}{1+\\exp^{-z}}$\n",
    "\n",
    "Example walk through\n",
    "\n",
    "\\begin{align*} \n",
    "&a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3) \\newline \n",
    "&a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3) \\newline \n",
    "&a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3) \\newline \n",
    "h_\\Theta(x) = &a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)} + \\Theta_{13}^{(2)}a_3^{(2)}) \\newline \n",
    "\\end{align*}\n",
    "\n",
    "If the network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\\Theta^j$ will be of dimension $s_{j+1} \\times (s_j + 1)$.  IE, with this simple example at j = 2, s_j = 3, s_j+1 = 1 -> dimension = 1 x 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rewrite the $a_1^{(2)}$ as $a_1^{(2)} = g(z_1^{(2)})$, allows us to use vectorized implementation.\n",
    "\n",
    "$z^{(2)} = \\Theta^{(1)} a^{(1)}$ and $a^{(2)} = g(z^{(2)})$\n",
    "\n",
    "To add the bias unit $a_0^{(2)} = 1$, now $a^{(2)}$ is 4D.\n",
    "\n",
    "$z^{(3)} = \\Theta^{(2)} a^{(2)}$\n",
    "\n",
    "$h_\\Theta = a^{(3)} = g(z^{(3)})$\n",
    "\n",
    "Process of computing $h(x)$ is called forward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}a_1^{(2)} = g(z_1^{(2)}) \\newline a_2^{(2)} = g(z_2^{(2)}) \\newline a_3^{(2)} = g(z_3^{(2)}) \\newline \\end{align*}\n",
    "\n",
    "In other words, for layer j=2 and node k, the variable z will be:\n",
    "\n",
    "z_k^{(2)} = \\Theta_{k,0}^{(1)}x_0 + \\Theta_{k,1}^{(1)}x_1 + \\cdots + \\Theta_{k,n}^{(1)}x_n\n",
    "\n",
    "The vector representation of $x$ and $z^j$ is:\n",
    "\n",
    "\\begin{align*}x = \\begin{bmatrix}x_0 \\newline x_1 \\newline\\cdots \\newline x_n\\end{bmatrix} &z^{(j)} = \\begin{bmatrix}z_1^{(j)} \\newline z_2^{(j)} \\newline\\cdots \\newline z_n^{(j)}\\end{bmatrix}\\end{align*}\n",
    "\n",
    "Setting $x = a^{(1)}$, rewrite:\n",
    "\n",
    "$z^{(j)} = \\Theta^{(j-1)}a^{(j-1)}$\n",
    "\n",
    "now:\n",
    "\n",
    "$a^{(j)} = g(z^{(j)})$, where function g is applied element wide to vector $z^{(j)}$.\n",
    "\n",
    "Next, add bias unit (equal to 1) to layer $j$ after computing $a^{(j)}$.  This is element $a_0^{(j)} = 1$.\n",
    "\n",
    "Finally:\n",
    "\n",
    "$z^{(j+1)} = \\Theta^{(j)} a^{(j)}$\n",
    "\n",
    "The last $\\Theta$ matrix will have only **one row** which is multiplied by a **single column** $a^{(j)}$ that yields a single number.\n",
    "\n",
    "Final result calculated with:\n",
    "\n",
    "$h_\\Theta(x) = a^{(j+1)} = g(z^{(j+1)})$\n",
    "\n",
    "This final step is the same performed in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is similar to logistic regression except it is using the inner layers which are determined by the features in layers above it.  This allows the network to find better estimates for useful features and is not limited to the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples and Intuitions\n",
    "\n",
    "#### Simple AND example\n",
    "\n",
    "Can we get a one unit example to illustrate an AND example\n",
    "\n",
    "$\\Theta^{(1)} = [-30, 20, 20]$, $x_0 = 1$\n",
    "\n",
    "-> \n",
    "\n",
    "$h_\\Theta(x) = g(-30 + 20 x_1 + 20 x_2)$\n",
    "\n",
    "Sigmoid landmarks: $g(4.6) = 0.99$, $g(-4.6) = 0.01$\n",
    "\n",
    "For $x_1 = [0, 0, 1, 1]$ and $x_2 = [0, 1, 0, 1]$\n",
    "\n",
    "$h_\\Theta(x) = [g(-30), g(-10), g(-10), g(1)] ~= [0, 0, 0, 1]$\n",
    "\n",
    "$h_\\Theta^{(x)} ~= x_1$ AND $x_2$\n",
    "\n",
    "An example of $x_1$ OR $x_2$ uses $\\Theta^{(1)} = [-10, 20, 20]$\n",
    "\n",
    "An example of NOT $x_1$ uses $\\Theta = [10, -20]$\n",
    "\n",
    "An example of NOT $x_1$ and NOT $x_2$ uses $\\Theta = [10, -20, -20]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification : XNOR\n",
    "\n",
    "* XNOR ~ Not X1 or X2\n",
    "\n",
    "Putting the above together with an input layer, one hidden layer, and an output layer: $a_1^{(2)} =$ $x_1$ AND $x_2$, $a_2^{(2)} =$ NOT $x_1$ AND NOT $x_2$, and $a_1^{(3)} =$ $x_1$ OR $x_2$.  This yields a $h_\\Theta^{(x)} ~= x_1$ XNOR $x_2$\n",
    "\n",
    "This illustrates that relatively simple functions can be layered to generate complex results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output determines which classification the image belongs to.  This requires multiple training sets for each classification.  Previously $y=[1, 2, 3, 4]$ now we want to associate an image with a classification.  $(x^{(i)},y^{(i)})$ is now $[1;0;0;0] , [0;1;0;0], [0;0;1;0], [0;0;0;1]$ and the image is $x^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exp:\n",
    "1. Any logical fucntion can be represented by neural, XOR is 3 layers, hidden activations defined by sigmoid range between 0-1, a1+a2+a3 != 1 always\n",
    "2. NAND [30,-20,-20]\n",
    "3. \n",
    "4. z = Theta1 * x; a2 = sig(z); or a2 = sig(x * Theta1), wrong\n",
    "5. Does not stay the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
