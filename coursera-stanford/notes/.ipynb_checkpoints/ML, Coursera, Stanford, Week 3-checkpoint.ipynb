{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classification and Representation\n",
    "\n",
    "## Classification\n",
    "* Examples\n",
    "    * Email: Spam / Not Spam\n",
    "    * Transaction Fraud: Yes / No\n",
    "* Usually applying linear regression to a classification problem will not provide good results\n",
    "\n",
    "### Binary Classification Problem\n",
    "$y = {0, 1}$\n",
    "* 0 ~ Negative Class, generally conveys the abscense of something\n",
    "* 1 ~ Positive Class\n",
    "* Linear regression returns a value not bounded by 0 and 1.  Logistic Regression is used instead for classification problems.\n",
    "\n",
    "\n",
    "\n",
    "## Hypothesis Representation\n",
    "* The Hypothesis representation of Logistic Regression.\n",
    "* Sigmoid and Logistic function labels are interchangable\n",
    "* This model is bounded by 0 and 1\n",
    "* Interpretation: the result of $h_\\theta(X)$ is the probability the result is 1.  More formally $h_\\theta(x) = P(y=1|x;\\theta)$\n",
    "* Since $y = 0;1$, $P(y=0|x;\\theta) = 1 - P(y=1|x;\\theta)$\n",
    "\n",
    "$ 0 \\leq h_\\theta(x) \\leq 1 $\n",
    "\n",
    "Use: $h_\\theta(x) = g(\\theta^T x)$, where $g(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "This leads to: $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}$\n",
    "\n",
    "## Decision Boundary\n",
    "* Understand better what the hypothesis function looks like\n",
    "* Suppose: predect $y=1$ if $h_\\theta(x) \\geq 0.5$ and $y=0$ for $h_\\theta(x) < 0.5$.\n",
    "    * then $g(z) \\geq 0.5$ when $z \\geq 0$\n",
    "    * leads to $y=1$ when $\\theta^T x \\geq 0$ and $y=0$ when $\\theta^T x < 0$\n",
    "* The 'decision boundary' separates the $y=0$ and $y=1$ values.\n",
    "* The decision boundary is a property of the hypothesis and not the data set.\n",
    "* The decision boundary does not need to be linear.  This can be accomplished by adding higher order features.  For example: $h_\\theta(x) = g(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_1^2 + \\theta_4 x_2^2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Logistic Regression Model\n",
    "The same cost function can not be used from the Linear regression model.\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "$\\begin{align*}\n",
    "& J(\\theta) = \\dfrac{1}{m} \\sum_{i=1}^m \\mathrm{Cost}(h_\\theta(x^{(i)}),y^{(i)}) \\\\\n",
    "& \\mathrm{Cost}(h_\\theta(x),y) = -\\log(h_\\theta(x)) \\; & \\text{if y = 1} \\\\ \n",
    "& \\mathrm{Cost}(h_\\theta(x),y) = -\\log(1-h_\\theta(x)) \\; & \\text{if y = 0}\n",
    "\\end{align*}$\n",
    "\n",
    "If $y=1$ the cost goes to $0$ as $h_\\theta(x)$ approaches $1$ and if $y=0$ the cost goes to $\\inf$ as $h_\\theta(x)$ approaches $1$. \n",
    "\n",
    "$\\begin{align*}\n",
    "& \\mathrm{Cost}(h_\\theta(x),y) = 0 \\text{ if } h_\\theta(x) = y \\newline \n",
    "& \\mathrm{Cost}(h_\\theta(x),y) \\rightarrow \\infty \\text{ if } y = 0 \\; \\mathrm{and} \\; h_\\theta(x) \\rightarrow 1 \\newline \n",
    "& \\mathrm{Cost}(h_\\theta(x),y) \\rightarrow \\infty \\text{ if } y = 1 \\; \\mathrm{and} \\; h_\\theta(x) \\rightarrow 0 \\newline \n",
    "\\end{align*}$\n",
    "\n",
    "Note that writing the cost function in this way guarantees that $J(\\theta)$ is convex for logistic regression.\n",
    "\n",
    "## Simplified Cost Function\n",
    "* A slightly simpler way to write the cost function\n",
    "    * Compresses the two lines where $y = 1,0$ into one\n",
    "    * $\\mathrm{Cost}(h_\\theta(x),y) = - y \\; \\log(h_\\theta(x)) - (1 - y) \\log(1 - h_\\theta(x))$\n",
    "* Using the gradient descent to fit the parameters of logic regression\n",
    "\n",
    "Entire cost function, derived from statistics using the principle of maximum likelyhood estimation.\n",
    "\n",
    "$J(\\theta) = - \\frac{1}{m} \\displaystyle \\sum_{i=1}^m [y^{(i)}\\log (h_\\theta (x^{(i)})) + (1 - y^{(i)})\\log (1 - h_\\theta(x^{(i)}))]$\n",
    "\n",
    "* need to find parameters of $\\theta$ that minimize the cost function $J(\\theta)$\n",
    "* Minimizing the cost function using Gradient Descent\n",
    "\n",
    "$\\begin{align*}& Repeat \\; \\lbrace \\newline & \\; \\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) \\newline & \\rbrace\\end{align*}$\n",
    "\n",
    "Working out the derivative:\n",
    "\n",
    "$\\begin{align*} & Repeat \\; \\lbrace \\newline & \\; \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\newline & \\rbrace \\end{align*}$\n",
    "\n",
    "This algorithm is identical to the one from linear regression.  The different is the hypothesis function.  In linear regression $h_{\\theta}(x) = \\theta^T x$ while here the hypothesis $h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^T x}}$ \n",
    "\n",
    "### Vecotrized implementation\n",
    "\n",
    "$\\begin{align*} & h = g(X\\theta)\\newline & J(\\theta) = \\frac{1}{m} \\cdot \\left(-y^{T}\\log(h)-(1-y)^{T}\\log(1-h)\\right) \\end{align*}$\n",
    "\n",
    "$\\theta := \\theta - \\frac{\\alpha}{m} X^{T} (g(X \\theta ) - \\vec{y})$\n",
    "\n",
    "## Advanced Optimization\n",
    "* Get logistic regression to run much faster than is possible with gradient descent\n",
    "* Different approaches to optimizing the cost function\n",
    "    * Conjugate gradient, BFGD, L-BFGS\n",
    "    * No need to pick $\\alpha$ but more complex and difficult to debug\n",
    "    * Recommends using the octave library to implement these\n",
    "    \n",
    "Example\n",
    "\n",
    "$J(\\theta) = (\\theta_1-5)^2 + (\\theta_2 - 5)^2$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_1} J(\\theta) = 2(\\theta_1 - 5)$\n",
    "$\\frac{\\partial}{\\partial \\theta_2} J(\\theta) = 2(\\theta_2 - 5)$\n",
    "\n",
    "\n",
    "Octave Code:\n",
    "\n",
    "    ''''\n",
    "    \n",
    "    function [jVal, gradient] = costFunction(theta)\n",
    "\n",
    "        jVal = [...code to compute J(theta)...];\n",
    "        # jVal = (theta(1) - 5)^2 + (theta(2) - 5)^2;\n",
    "\n",
    "        gradient = [...code to compute derivative of J(theta)...];\n",
    "        # gradient = zeros(2,1)\n",
    "        # gradient(1) = 2*(theta(1) - 5);\n",
    "        # gradient(2) = @*(theta(2) - 5);\n",
    "\n",
    "    end\n",
    "\n",
    "    ''''\n",
    "\n",
    "    ''''\n",
    "\n",
    "    options = optimset('GradObj', 'on', 'MaxIter', 100);\n",
    "\n",
    "    initialTheta = zeros(2,1);\n",
    "\n",
    "         [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta,\n",
    "         options);\n",
    "\n",
    "    ''''\n",
    "\n",
    "* @costFunction is a pointer to the costFunction\n",
    "* exit flag of 1 detaisl the convergence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multiclass Classification\n",
    "* Examples: email foldering/tagging (work, friends, family, hobby), medical diagrams (not ill, cold, flu), weather (sunny, cloudy, rain, snow)\n",
    "* y takes on a small number of discrete values\n",
    "\n",
    "## One-vs-All\n",
    "\n",
    "* aka 'one vs rest'\n",
    "* seperates the data set into binary classification problems of the class to solve for vs the rest of the set.\n",
    "* trains a logistic regression classifier $h_\\theta(x)$ for each class to predict the probability that $y=i$.  \n",
    "* $y$ for a new value $x$ is predicted to be the value $i$ where $h_\\theta^{(i)} (x)$ is a maximum\n",
    "\n",
    "$\\begin{align*}& y \\in \\lbrace0, 1 ... n\\rbrace \\newline& h_\\theta^{(0)}(x) = P(y = 0 | x ; \\theta) \\newline& h_\\theta^{(1)}(x) = P(y = 1 | x ; \\theta) \\newline& \\cdots \\newline& h_\\theta^{(n)}(x) = P(y = n | x ; \\theta) \\newline& \\mathrm{prediction} = \\max_i( h_\\theta ^{(i)}(x) )\\newline\\end{align*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x857f4f0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x81862d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = sp.arange(-10,11,1)\n",
    "plt.plot(x, 1/(1+sp.exp(x)))\n",
    "plt.plot([-10, 10], [0.5, 0.5], 'k')\n",
    "plt.plot([0, 0], [0, 1], 'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Solving the Problem of Overfitting\n",
    "* Overfitting can cause algorithms to perform very poorly\n",
    "* Regularization should allow us ot reduce this problem\n",
    "\n",
    "## The Problem\n",
    "* Underfitting occurs when a line through the data does not correlate well with the results.  This leads to a \"high bias\"\n",
    "* The other end of the extreme occurs when your number of $\\theta$s approaches the number of results.  This leads to a result that fits closely the existing data, but would poorly predict new results.  This leads to \"high varience\".\n",
    "* Addressing overfitting: \n",
    "    1. Manually select which features to keep, model selection algorithm\n",
    "    2. Regularization: keep all the features but reduce the magnitude/values of parameters $\\theta_j$.  Works well when we have lots fo features, each of which contributes to predicting $y$.\n",
    "    \n",
    "## Cost Function\n",
    "\n",
    "Example of penalizing $\\theta_3$ and $\\theta_4$.\n",
    "\n",
    "$min_\\theta\\ \\dfrac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 + 1000\\cdot\\theta_3^2 + 1000\\cdot\\theta_4^2$\n",
    "\n",
    "Modify the cost function to add regularization term to shrink all of the parameters\n",
    "\n",
    "$min_\\theta\\ \\dfrac{1}{2m}\\  \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\ \\sum_{j=1}^n \\theta_j^2$.\n",
    "\n",
    "$\\lambda$ is the *regularization parameter*.  It determines how much the $\\theta$ parameters are inflated.  A large $\\lambda$ will smooth out the function and cause underfitter.  A $\\lambda = 0$ would eliminate the regularization parameter and could lead to overfitting.\n",
    "\n",
    "## Regularized Linear Regression\n",
    "Normalizing the learning algorithms gradient descent and the normal equation for the case of regularized linear regression.\n",
    "\n",
    "### Linear Descent\n",
    "\n",
    "$\\begin{align*} & \\text{Repeat}\\ \\lbrace \\newline & \\ \\ \\ \\ \\theta_0 := \\theta_0 - \\alpha\\ \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\newline & \\ \\ \\ \\ \\theta_j := \\theta_j - \\alpha\\ \\left[ \\left( \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\right] &\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j \\in \\lbrace 1,2...n\\rbrace\\newline & \\rbrace \\end{align*}$\n",
    "\n",
    "The case for $\\theta_0$ written seperately to not penalize $\\theta_0$.\n",
    "\n",
    "Can be re-written as\n",
    "\n",
    "$\\theta_j := \\theta_j(1 - \\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "\n",
    "The term $1 - \\alpha \\frac{\\lambda}{m}$ is always less then $1$.  \n",
    "\n",
    "### Normal Equation\n",
    "\n",
    "$\\begin{align*}& \\theta = \\left( X^TX + \\lambda \\cdot L \\right)^{-1} X^Ty \\newline& \\text{where}\\ \\ L = \\begin{bmatrix} 0 & & & & \\newline & 1 & & & \\newline & & 1 & & \\newline & & & \\ddots & \\newline & & & & 1 \\newline\\end{bmatrix}\\end{align*}$\n",
    "\n",
    "The formula for $\\theta$ above minmizes the cost function.\n",
    "\n",
    "'\n",
    "    Recall that if m < n, then XTX is non-invertible. However, when we add the term λ⋅L, then XTX + λ⋅L becomes invertible. Assumes $\\lambda > 0$.\n",
    "'\n",
    "\n",
    "## Regularized Logistic Regression\n",
    "\n",
    "Adapt both gradient descent and the more advanced optimiztation techniques in order to have them work for regularized logistic regression.\n",
    "\n",
    "Add the $\\lambda$ term at the end to apply regularization to logistic regression.\n",
    "\n",
    "$J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\large[ y^{(i)}\\ \\log (h_\\theta (x^{(i)})) + (1 - y^{(i)})\\ \\log (1 - h_\\theta(x^{(i)}))\\large] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2$\n",
    "\n",
    "The second summation is meant to exclude $\\theta_0$.\n",
    "\n",
    "The gradient descent equation has the same form with the $\\lambda$ term added at the end.  The hypothesis term however is different as we saw previously.\n",
    "\n",
    "### Using Octave\n",
    "\n",
    "''''\n",
    "\n",
    "    function [jVal, gradient] = costFunction(theta)\n",
    "        jVal = [code to compute J(theta)];\n",
    "        \n",
    "        gradient(1) = [code to compute partial/partial_theta_0 J(theta)];\n",
    "        gradient(2) = [code to compute partial/partial_theta_2 J(theta)];\n",
    "        \n",
    "        gradient(n+1) = [code to compute partial/partial_theta_n J(theta)];\n",
    "\n",
    "''''\n",
    "\n",
    "Calculating the gradient on $\\theta_1$ to $\\theta_n$ requires the $\\lambda$ term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
