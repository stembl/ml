{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Notes\n",
    "## Stanford Coursera Class\n",
    "Week One - 4/7/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "Where the \"right answers\" are given.\n",
    "\n",
    "For example:\n",
    "* Housing prices based on square foot\n",
    "* Stock market, volume of shares traded in a day\n",
    "* Will it rain on a given day\n",
    "* Based on tumor size, is it malignant or benign\n",
    "* Is an author male or female based on previous works\n",
    "* Final height and weight of a child based on historical data\n",
    "\n",
    "Supervised learning problems are categorized into Regression and Classification problems.\n",
    "\n",
    "#### Regression \n",
    "Problem where we are trying to predict a continuous value output. Mapping input variables onto a continuous function.\n",
    "\n",
    "For example:\n",
    "* How many items will sell over the next 3 months\n",
    "* Predicting price of a house based on square footage.  Price is a function of size.\n",
    "* Predicting a persons age based on a given picture\n",
    "\n",
    "#### Classification\n",
    "Problem where we are trying to predict a discrete valued output\n",
    "\n",
    "For example:\n",
    "* Predicting whether a house will sell for more (1) or less(0) than an asking price.\n",
    "* Is the tumor malignant (1) or benign (0).\n",
    "* Is the account hacked or not\n",
    "\n",
    "A Learning algorithm can deal with an infinite number of attributes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "* Data all has the same label.  \"Here is the data set, can you find some structure in the data?\"\n",
    "* Allows you to approach a problem with little to no idea what the result should look like.\n",
    "* Given a dataset, an algorithm may determine the data is clustered.  This is called a clustering algorithm.\n",
    "* Non-clustering (aka Cocktail Party Algorithm) allows you to find structure in a chaotic environment.\n",
    "\n",
    "#### Clustering Algorithm\n",
    "\n",
    "For example:\n",
    "* Google News.  Google looks at 1000's of stories and clusters them based on a theme.  ie BP oil spill\n",
    "* DNA Microarray.  Grouping individuals based on whether a gene is present.  The algorithm is not told information about the people. \n",
    "* Social Network Analysis. Google groups identifies cohesive gorups of friends based on people you email the most.\n",
    "* Market Segmentation.  Given customer data, but not what market segment the customer lies in.\n",
    "* Astronomical Data Analysis.  \n",
    "\n",
    "#### Cocktail Party\n",
    "Clusters the signal received and tries to differentiate the sources.\n",
    "\n",
    "?What are the variables it is using to compute this?\n",
    "\n",
    "\n",
    "Cocktail party examples performed with a single line of code:\n",
    "\n",
    "$[W,s,v] = svd((repmat(sum(x.*x,1)size(x,1),1).*x)*x')$\n",
    "\n",
    "#### Octave\n",
    "Software often implemented in octave first.  The algorithm svd() is simple in Octave as opposed to C++ or java.  In R, many of the ML techiques are already implemented at a high level reducing the learning opportunity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Representation\n",
    "\n",
    "* Our first algorithm will be Linear Regression.\n",
    "\n",
    "Notation:\n",
    "* m   ~  Number of training examples\n",
    "* $x^{(i)}$ ~ \"Input\" variable / features\n",
    "* $y^{(i)}$ ~ \"Output\" variable / features\n",
    "* $(x^{(i)},y^{(i)})$ ~ a training example\n",
    "* $(x^{(i)},y^{(i)}); i=1,...,m$ ~ a dataset or list of $m$ training examples\n",
    "* $X$ ~ denotes the space of input values\n",
    "* $Y$ ~ denotes the space of output values\n",
    "* $h$ ~ Hypothesis.  The hypothesis takes the input and trys to estimate the price.\n",
    "\n",
    "For the next few videos the hypthoesis will be represented as:\n",
    "\n",
    "$h_\\theta(x) = \\theta_0 + \\theta_1 x$, shorthand: $h(x)$\n",
    "\n",
    "* This model is called univariate linear regression\n",
    "* Univariate means a single variable\n",
    "\n",
    "For a supervised learning problem, our goal, given a training set, is to learn a function $h:X\\rightarrow Y$ so that $h(x)$ is a \"good\" predictor for the corresponding value of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "* Given a hypothesis, $h_\\theta(x) = \\theta_0 + \\theta_1 x$, the parameters are $\\theta_0$ and $\\theta_1$.\n",
    "* To choose parameters, solve the minimization problem:\n",
    "    * minimize $\\theta_0, \\theta_1$\n",
    "    * $\\frac{1}{2m}\\sum\\limits^m_{i=1} (h_\\theta(x^{(i)}) - y^{(i)})^2$, $h_\\theta(x) = \\theta_0 + \\theta_1 x$\n",
    "    * By convention, the cost function: $J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum\\limits^m_{i=1} (h_\\theta(x^{(i)}) - y^{(i)})^2$, $\\underset{\\theta_0,\\theta_1}{\\text{minimize}}$ $J(\\theta_0,\\theta_1)$.\n",
    "    * The squared error cost function is a reasonalbe choice for a regression problem. aka mean squared error function.\n",
    "    * The $1/2m$ constant at the front doesn't change the value, but simplifies the problem\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function Intuition\n",
    "Previously:\n",
    "\n",
    "Hypothesis: $h_\\theta(x) = \\theta_0 + \\theta_1 x$\n",
    "\n",
    "Parameters: $\\theta_0, \\theta_1$\n",
    "\n",
    "Cost Function: $J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum\\limits^m_{i=1} (h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "Goal: $\\underset{\\theta_0,\\theta_1}{\\text{minimize}}$ $J(\\theta_0,\\theta_1)$\n",
    "\n",
    "For this example we will use a simplified hypothesis.\n",
    "\n",
    "Hypothesis: $h_\\theta(x) = \\theta_1 x$\n",
    "\n",
    "Parameters: $\\theta_1$\n",
    "\n",
    "Cost Function: $J(\\theta_1)=\\frac{1}{2m}\\sum\\limits^m_{i=1} (h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "Goal: $\\underset{\\theta_1}{\\text{minimize}}$ $J(\\theta_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple Hypothesis Example\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "def simph(theta_1, exp):\n",
    "    m = exp.shape[0]\n",
    "    h = sp.zeros(m)\n",
    "    for i in range(m):\n",
    "        h[i] = theta_1*exp[i,0]\n",
    "    \n",
    "    return(h)\n",
    "\n",
    "def simpj(theta, exp):\n",
    "    '''\n",
    "    theta ~ values of theta to explore\n",
    "    exp ~ example set\n",
    "    '''\n",
    "    m = exp.shape[0]\n",
    "    J = sp.zeros(len(theta))\n",
    "    \n",
    "    for k in range(len(theta)):\n",
    "        h = simph(theta[k], exp)\n",
    "        for i in range(m):\n",
    "            J[k] = J[k] + (h[i] - exp[i,1])**2\n",
    "        J[k] = (1/(2*m))*J[k]\n",
    "    \n",
    "    return(J)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J is a minimum at theta = 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x772cfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp = sp.array([[1,1],[2,2],[3,3]])\n",
    "\n",
    "theta = sp.linspace(-0.5, 2.5, 99)\n",
    "\n",
    "J = simpj(theta,exp)\n",
    "\n",
    "plt.plot(theta, J)\n",
    "\n",
    "print('J is a minimum at theta = {:1}' .format(theta[J==min(J)][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Parameter Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient Descent\n",
    "* The gradient descent is an algorithm used all over the place in machine learning\n",
    "* This algorithm works for general problems up to $\\theta_n$, but we are only looking at two variables for this explanation\n",
    "* Adjust alpha to ensure the gradient descent converges in a reasonable time\n",
    "* As a local minima is approached the gradient descent automatically takes smaller steps, no need to decrease $\\alpha$ over time\n",
    "\n",
    "\n",
    "Terminology\n",
    "* := ~ Assignment, a := b -> set the value of a equal to b\n",
    "* $\\alpha$ ~ Learning rate, how big of steps are taken\n",
    "* m ~ size of the training set\n",
    "\n",
    "\n",
    "#### Gradient Descent Algorithm\n",
    "\n",
    "Repeat until convergence:\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)$,\n",
    "\n",
    "where $j=0,1$ represents the feature index number.\n",
    "\n",
    "\n",
    "Correct: Simultaneous update\n",
    "1. $temp0 := \\theta_0 - \\alpha \\frac{\\partial}{\\partial\\theta_0}J(\\theta_0,\\theta_1)$\n",
    "2. $temp1 := \\theta_1 - \\alpha \\frac{\\partial}{\\partial\\theta_1}J(\\theta_0,\\theta_1)$\n",
    "3. $\\theta_0 := temp0$\n",
    "4. $\\theta_1 := temp1$\n",
    "\n",
    "#### Gradient Descent for Linear Regression\n",
    "* aka \"Batch\" gradient descent, because you look at the entire training set as opposed to small subsets of the data.'\n",
    "* Gradien descent scales better than the normal equations method\n",
    "\n",
    "Repeat until convergence:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\theta_0 := &\\theta_0 - \\alpha \\frac{1}{m}\\sum\\limits^m_{i=1} (h_\\theta(x_i) - y_i),\\\\\n",
    "\\theta_1 := &\\theta_1 - \\alpha \\frac{1}{m}\\sum\\limits^m_{i=1} ((h_\\theta(x_i) - y_i)x_i),\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "where $m$ is the size of the training set, $\\theta_0$ a constant that will be changing simultaneously with $\\theta_1$, and $x_i$, $y_i$ are values of the given training set (data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
